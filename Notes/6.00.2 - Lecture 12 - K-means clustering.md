# K-means clustering

Starts with a set X and a predefined number of clusters k.
Partition X into k clusters such that it approximately minimises the objective function.
```
Outer loop: for c in k clusters:
     Inner loop: for x in set X:
          minimise sum of square of value x - mean µ of cluster c (distance from each point x to the centre of the cluster). 
```
Minimise the sum of mean square differences. 

K-means algorithms:
randomly choose k examples as centroids.

Odds ratio = Actual % / Expected %

## Scaling

Features have different means and variance.
Rescaled feature:
```
x' = (x - µ) / σ
```

```
while true: 
  create k clusters by assigning each example to closest  centroid.
  compute k new centroids by averaging examples in each cluster. 
    if centroids don't change:
      break
```
## Issues:

   * final result can depend on initial centroid. Greedy algorithm can find different local optima
   * choosing the wrong k can lead to nonsense. 

```
best = kMeans(points)
for t in range(numTrials):
     C = kMenas(points)
     if badness(C) < badness(best):
          best = C
```

## Hierarchical 
* looks at different nr of clusters from 1 to N
* is slow
* is deterministic

## K-means
* looks at many ways of creating clusters
* fast
* non-deterministic
